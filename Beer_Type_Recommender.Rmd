---
title: "Beer_Type_Recommender"
author: "Edward DeWane"
date: "2024-02-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{python}
#this is to see if my model can accurately predict the type of beer
import pandas as pd
import pyarrow
import torch
import transformers
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertModel, DistilBertTokenizer


from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'
# device = "cpu"

df = pd.read_csv('englishreviews.csv')
df3 = pd.read_csv('beers.csv')
df3 = df3.assign(beertype=['Stout', 'Pale Ale', 'English Bitter', 'Pale Ale', 'Pale Ale', 'Saison', 'Dubbel', 'Stout', 'Kolsch', 'Brown Ale', 'Pale Ale', 'Saison', 'Brown Ale', 'Bière de Garde', 'Stout', 'Stout', 'Barleywine', 'Rye', 'Pale Ale', 'Pale Ale', 'Wild Ale', 'Pale Ale', 'Pale Ale', 'Pale Ale', 'Stout', 'Stout', 'Wild Ale', 'Flanders Oud Bruin', 'English Bitter', 'English Dark Mild Ale'])

df2 = pd.merge(df,df3,how = 'left', on = 'beername')

df2['beertype'] = pd.factorize(df2['beertype'])[0]

df2 = df2.dropna(subset=['beertype'])

df2['beertype'].unique()

df2[df2['beertype']==-1]

df2 = df2[df2['beertype']!=-1]

df2['beertype'].unique()

MAX_LEN = 512
TRAIN_BATCH_SIZE = 8
VALID_BATCH_SIZE = 4
EPOCHS = 10
LEARNING_RATE = 1e-05
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')

class Triage(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.len = len(dataframe)
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __getitem__(self, index):
        review = str(self.data.review[index])
        review = " ".join(review.split())
        inputs = self.tokenizer.encode_plus(
            review,
            None,
            add_special_tokens = True,
            max_length = self.max_len,
            padding = 'max_length',
            return_token_type_ids = True,
            truncation = True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']

        return {
            'ids': torch.tensor(ids, dtype = torch.long),
            'mask': torch.tensor(mask, dtype = torch.long),
            'targets': torch.tensor(self.data.beertype[index], dtype = torch.long)
        }

    def __len__(self):
        return self.len
      
      
train_size = 0.8
train_dataset = df2.sample(frac = train_size, random_state = 200)
test_dataset = df2.drop(train_dataset.index).reset_index(drop = True)
train_dataset = train_dataset.reset_index(drop = True)


print("FULL Dataset: {}".format(df2.shape))
print("TRAIN Dataset: {}".format(train_dataset.shape))
print("TEST Dataset: {}".format(test_dataset.shape))

training_set = Triage(train_dataset, tokenizer, MAX_LEN)
testing_set = Triage(test_dataset, tokenizer, MAX_LEN)

train_params = {'batch_size': TRAIN_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
                }

test_params = {'batch_size': VALID_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
                }

training_loader = DataLoader(training_set, **train_params)
testing_loader = DataLoader(testing_set, **test_params)


class DistillBERTClass(torch.nn.Module):
    def __init__(self):
        super(DistillBERTClass, self).__init__()
        self.l1 = DistilBertModel.from_pretrained("distilbert-base-uncased")
        self.pre_classifier = torch.nn.Linear(768, 768)
        self.dropout = torch.nn.Dropout(0.2)
        self.classifier = torch.nn.Linear(768, 13)

    def forward(self, input_ids, attention_mask):
        output_1 = self.l1(input_ids = input_ids, attention_mask = attention_mask)
        hidden_state = output_1[0]
        pooler = hidden_state[:, 0]
        pooler = self.pre_classifier(pooler)
        pooler = torch.nn.ReLU()(pooler)
        pooler = self.dropout(pooler)
        output = self.classifier(pooler)
        return output
      
model = DistillBERTClass()
model.to(device)


loss_function = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)


def calcuate_accu(big_idx, targets):
    n_correct = (big_idx==targets).sum().item()
    return n_correct
  
def train(epoch):
    tr_loss = 0
    n_correct = 0
    nb_tr_steps = 0
    nb_tr_examples = 0
    model.train()
    for _,data in enumerate(training_loader, 0):
        ids = data['ids'].to(device, dtype = torch.long)
        mask = data['mask'].to(device, dtype = torch.long)
        targets = data['targets'].to(device, dtype = torch.long)

        outputs = model(ids, mask)
        loss = loss_function(outputs, targets)
        tr_loss += loss.item()
        big_val, big_idx = torch.max(outputs.data, dim=1)
        n_correct += calcuate_accu(big_idx, targets)

        nb_tr_steps += 1
        nb_tr_examples+=targets.size(0)

        if _%5000==0:
            loss_step = tr_loss/nb_tr_steps
            accu_step = (n_correct*100)/nb_tr_examples
            print(f"Training Loss per 5000 steps: {loss_step}")
            print(f"Training Accuracy per 5000 steps: {accu_step}")

        optimizer.zero_grad()
        loss.backward()
        # # When using GPU
        optimizer.step()

    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')
    epoch_loss = tr_loss/nb_tr_steps
    epoch_accu = (n_correct*100)/nb_tr_examples
    print(f"Training Loss Epoch: {epoch_loss}")
    print(f"Training Accuracy Epoch: {epoch_accu}")

    return  
  
for epoch in range(EPOCHS):
    train(epoch)
    
    
def valid(model, testing_loader):
    model.eval()
    tr_loss = 0
    n_correct = 0
    nb_tr_steps = 0
    nb_tr_examples = 0
    n_wrong = 0
    total = 0
    with torch.no_grad():
        for _, data in enumerate(testing_loader, 0):
            ids = data['ids'].to(device, dtype = torch.long)
            mask = data['mask'].to(device, dtype = torch.long)
            targets = data['targets'].to(device, dtype = torch.long)
            outputs = model(ids, mask).squeeze()
            loss = loss_function(outputs, targets)
            tr_loss += loss.item()
            big_val, big_idx = torch.max(outputs.data, dim=1)
            n_correct += calcuate_accu(big_idx, targets)

            nb_tr_steps += 1
            nb_tr_examples+=targets.size(0)

            if _%5000==0:
                loss_step = tr_loss/nb_tr_steps
                accu_step = (n_correct*100)/nb_tr_examples
                print(f"Validation Loss per 100 steps: {loss_step}")
                print(f"Validation Accuracy per 100 steps: {accu_step}")
    epoch_loss = tr_loss/nb_tr_steps
    epoch_accu = (n_correct*100)/nb_tr_examples
    print(f"Validation Loss Epoch: {epoch_loss}")
    print(f"Validation Accuracy Epoch: {epoch_accu}")

    return epoch_accu


acc = valid(model, testing_loader)
print("Accuracy on test data = %0.2f%%" % acc)

beer_type = {0: 'Stout', 1: 'Pale Ale', 2: 'English Bitter', 3: 'Saison', 4: 'Dubbel', 5: 'Kolsch', 6: 'Brown Ale', 7: 'Bière de Garde', 8: 'Barleywine', 9: 'Rye', 10: 'Wild Ale', 11: 'Flanders Oud Bruin', 12: 'English Dark Mild Ale'}

def predict(model, testing_loader):
    with torch.no_grad():
        for _, data in enumerate(testing_loader, 0):
            ids = data['ids'].to(device, dtype = torch.long)
            mask = data['mask'].to(device, dtype = torch.long)
            targets = data['targets'].to(device, dtype = torch.long)
            outputs = model(ids, mask)

    return outputs

ex_text_str = "This beer was a little flat for my liking"
new_data = pd.DataFrame(data = {'review': [ex_text_str], 'beertype': 0})

new_set = Triage(new_data, tokenizer, MAX_LEN)

new_loader = DataLoader(new_set, **test_params)

model = model.to("cuda")

prediction = predict(model, new_loader)

print("This beer is a %s." %beer_type[prediction.argmax(1).item()])


```

